---
title: "Food Access & Nutrition Equity in Texas"
subtitle: "Python pandas - Project Workflow and Report"
date: today
author: 
  - Chi-Tse Chiang(cc79734)
  - Cian-Rong Chen(cc79648)
format:
  html:
    toc: true
    embed-resources: true
    number-sections: true
mainfont: TeX Gyre Schola
monofont: JetBrainsMono Nerd Font
mathfont: TeX Gyre Schola Math
---

# Summary

This project aims to develop a data wrangling pipeline that integrates three datasets related to **food access**, **nutritional quality**, and **socioeconomic disparities** in Texas. Using data from the ***USDA Food Access Research Atlas***, ***CORGIS County Demographics dataset***, and ***CORGIS Food Nutrition*** dataset. The pipeline is implemented across four technical environments:

- Python with Pandas
- R with tidyverse
- SQL
- Excel

The resulting clean dataset enables exploration of how food accessibility in Texas intersects with nutritional availability and demographic factors such as poverty or race/ethnicity. This work emphasizes the technical process of data wrangling and reproducible pipeline development, providing a foundation for future research into food security and health equity disparities across Texas communities.

# Data Sources

## USDA Food Access Research Atlas

**Source**: [USDA Data Products](https://www.ers.usda.gov/data-products/food-access-research-atlas)

![](FoodAccessResearchAtlas_datasource.png)
[72535rows x 147 columns]

Provides census-tract-level indicators of supermarket accessibility and food access challenges for different demographic groups. Includes population, housing, income, race, SNAP benefits, and geographic accessibility measures (e.g., low-income populations living >1 mile from a grocery store).

**Wrangling Issues**:

- Very wide (147 columns) and long (72,000+ rows) dataset requiring subsetting to 10–15 meaningful columns
- Needs standardization of county and state names for **merging**.
- Requires treatment of missing or 0 placeholder values
- Census tract-level data must be aggregated to county level to match other datasets

## U.S. County Demographics(From 2010s)

**Source**: [CORGIS Dataset Project, County Demographics](https://corgis-edu.github.io/corgis/csv/county_demographics/)

![](County_demographics.png)
[3140rows x 43 columns]

County-level data from 2010–2019 across the U.S., including age distribution, education, employment, ethnicity, household income, housing characteristics, and health-related statistics like travel time and veteran status.

**Wrangling Issues**:

- **Filtering** to only Texas counties from national dataset.
- **Missing values** encoded as -1 need identification and handling.
- Long dot-separated column names (e.g., Ethnicities.White Alone) require **renaming** and flattening for usability.
- County and state name standardization needed to **match** with USDA Food Access Atlas

## USDA Food Composition 

**Source**: [CORGIS Dataset Project, Food](https://corgis-edu.github.io/corgis/csv/food/)

![](Food_datasource.png)
[7084rows x 38columns] 

Contains nutritional breakdowns of thousands of foods, with fields for macronutrients (protein, fat, carbohydrates), vitamins (A, C, B12, etc.), and minerals (calcium, iron, magnesium). Each row represents a distinct food item.

**Wrangling Issues**:

- Contains 60+ nutrient columns requiring reduction to 5–10 most relevant variables.
- Food names contain **formatting synonyms** (e.g., "Milk, human" vs "Human milk") requiring text standardization.
- **Grouping** by food type (e.g., "Dairy", "Meat", "Vegetables") for analysis.
- Measurements use different units (grams, mg, mcg) that must be documented for proper interpretation.


# Columns Selected

## USDA Food Access Research Atlas (10 columns)

We retain these 11 columns from the original 147 to capture essential food access metrics while eliminating redundancy. The selected variables focus on the standard 1-mile threshold for urban areas and 10-mile threshold for rural areas, as this represents the USDA's primary food desert definition. We keep only the two largest racial/ethnic minority groups in Texas (Black and Hispanic populations) to enable disparity analysis without excessive granularity. Geographic identifiers are essential for merging datasets, while population totals serve as denominators for calculating meaningful access percentages at both tract and county levels.

|Column Name |Data Type|Description|Example|Notes|
|-----------------|------------|------------------------------|---------------------|----------------------------------|
|State|String|State name|"Texas"|Use for filtering; standardize to "TX" for merging|
|County|String|County name|"Harris County"|May need suffix standardization, merge key with Demographics|
|Urban|Integer (Binary)|Flag indicating if tract is urban (1) or rural (0)|0, 1|Based on Census Bureau urban area definitions; use for urban/rural analysis|
|PovertyRate|Float|Percentage of tract population living at or below federal poverty threshold|0.0 - 100.0 (typically 5-40)|Decimal format (e.g., 15.3 = 15.3%)|
|Pop2010|Integer|Total population count from 2010 Census|1,500 - 8,000 (typical tract)|Denominator for all percentage calculations; validate against Demographics dataset|
|TractLOWI|Integer|Total count of low-income population in tract|0 - 5,000|Denominator for low-income disparity calculations; low-income defined as ≤200% of poverty line|
|lapop1|Integer|Population count beyond 1 mile from supermarket|0 - 6,000|Use to calculate PercentLowAccess = (lapop1/Pop2010)*100; primary food access indicator|
|lalowi1|Integer|Low income population count beyond 1 mile from supermarket|0 - 4,000|Use to calculate PercentLowIncomeLowAccess = (lalowi1/TractLOWI)*100; measures vulnerable population access|
|lablack1|Integer|Black/African American population count with low access|0 - 3,000|Numerator for Black disparity ratio; compare to county-level PercentBlack|
|lahisp1|Integer|Hispanic/Latino population count with low access|0 - 4,000|Numerator for Hispanic disparity ratio; compare to county-level PercentHispanic|

## U.S. County Demographics (10 columns)

We select these 10 columns from the original 43 to provide socioeconomic context for food access patterns without overwhelming the analysis. The focus is on variables directly relevant to our research questions: income and education as economic indicators, age structure to identify vulnerable populations, and detailed racial/ethnic composition to enable disparity calculations. We exclude employment, housing, and business ownership variables as they are less directly related to food access outcomes. The 2010 population figures align temporally with the USDA food access data for valid comparisons.

|Column Name |Data Type|Description|Example|Notes|
|----------------------|--------------|--------------------------------------------|--------------|-----------------------------|
|County|String|County name|"Harris County"|Merge key with USDA, need to add/remove "County" suffix for consistency|
|State|String|State abbreviation or name|"TX" or "Texas"|Standardize to match USDA format ("TX" recommended)|
|Population.2010 Population|Integer|County population from 2010 Census|825 - 4,000,000|Rename to: Population2010|
|Population.Population per Square Mile|Float|Population density|1.5 - 3,000+|Rename to: PopDensity|
|Income.Median Houseold Income|Integer|Median household income (2015-2019 ACS)|$30,000 - $100,000+|Rename to: MedianIncome|
|Education.Bachelor's Degree or Higher|Float|Percentage of adults 25+ with bachelor's degree or higher (2015-2019 ACS)|8.0 - 60.0|Rename to: BachelorsDegreeRate|
|Age.Percent Under 18 Years|Float|Percentage of population under age 18|15.0 - 35.0|Rename to: PercentUnder18|
|Ethnicities.Black Alone|Float|Percentage of population identifying as Black/African American alone|0.5 - 50.0|Rename to: PercentBlack|
|Ethnicities.Hispanic or Latino|Float|Percentage of population identifying as Hispanic/Latino (any race)|5.0 - 95.0|Rename to: PercentHispanic, key disparity metric|
|Ethnicities.White Alone|Float|Percentage of population identifying as White alone|10.0 - 90.0|Rename to: PercentWhite|


## USDA Food Composition (9 columns)

We retain these 9 columns from the original 38 to create a focused nutritional profile without excessive micronutrient detail. The selection emphasizes macronutrients that define food quality—protein for satiety, fiber as a health indicator, and sugar as a marker of processed foods. We keep only two micronutrients (Vitamin A and calcium) as they are most commonly deficient in food desert populations and represent broader nutritional adequacy. This streamlined approach enables clear categorization of "nutrient-dense" versus "empty calorie" foods while avoiding the analytical complexity of tracking dozens of vitamins and minerals.

|Column Name |Data Type|Description|Example|Notes|
|------------------|--------|--------------------------------------------|--------------|------------------------------|
|Category|String|General food category assigned by USDA|"Milk", "Beef Product"| Use for grouping in nutrition|
|Description|String|Full description of food item|"Milk, whole, 3.25% milkfat"|May contain formatting inconsistencies|
|Data.Protein|Float|Protein content|0.0 - 90.0(g)|Rename to: Protein; high values in meat, fish, legumes| |Data.Carbohydrate|Float|Total carbohydrate content (by difference)|0.0 - 100.0(g)|Rename to: Carbohydrate; high in grains, fruits, sugars|
|Data.Fiber|Float|Dietary fiber content|0.0 - 40.0(g)|Rename to: Fiber; quality indicator|
|Data.Sugar Total|Float|Total sugar content|0.0 - 100.0(g)|Rename to: SugarTotal; quality indicator; high = worse (candies, sodas)|
|Data.Fat.Total Lipid|Float|Total fat content|0.0 - 100.0(g)|Rename to: TotalFat; high in oils, nuts, fatty meats|
|Data.Vitamins.Vitamin A - RAE|Integer|Vitamin A content as Retinol Activity Equivalents|0 - 20,000+(mcg)|Rename to: VitaminA; high in orange vegetables, dairy, liver|
|Data.Major Minerals.Calcium|Integer|Calcium content|0 - 2,000+(mg)|Rename to: Calcium; high in dairy, leafy greens, fortified foods|


# Data Loading

We now have:

- FoodAccessResearchAltas.csv
- county_demographics.csv
- food.csv

## Load csv 

```{python}
import pandas as pd

usda_atlas = pd.read_csv('FoodAccessResearchAltas.csv')
county_demographics = pd.read_csv('county_demographics.csv')
food_nutrition = pd.read_csv('food.csv')
```

```{python}
print("USDA Atlas shape:", usda_atlas.shape)
print("USDA Atlas columns:", usda_atlas.columns.tolist())
```

```{python}
print("County Demographics shape:", county_demographics.shape)
print("County Demographics columns:", county_demographics.columns.tolist())
```

```{python}
print("Food Nutrition shape:", food_nutrition.shape)
print("Food Nutrition columns:", food_nutrition.columns.tolist())
```

```{python}
usda_atlas.head()
```

```{python}
county_demographics.head()
```

```{python}
food_nutrition.head()
```

## States Selecting

```{python}
# Create a synonym mapping dictionary to standardize state names
state_synonym_map = {
    'TX': 'Texas',
    'texas': 'Texas',
    'TEXAS': 'Texas',
    'Texas': 'Texas',
    'CA': 'California',
    'california': 'California',
    'CALIFORNIA': 'California',
    'California': 'California',
    'NY': 'New York',
    'new york': 'New York',
    'NEW YORK': 'New York',
    'New york': 'New York'
}

# Apply the synonym mapping to standardize state names in usda_altas
usda_atlas['State'] = usda_atlas['State'].replace(state_synonym_map)

county_demographics['State'] = county_demographics['State'].replace(state_synonym_map)

print(f"usda_altas unique states: {usda_atlas['State'].nunique()}")
print(f"county_demographics unique states: {county_demographics['State'].nunique()}")
```

```{python}
usda_atlas = usda_atlas.query("State in ['Texas','California','New York']")
county_demographics = county_demographics.query("State in ['Texas','California','New York']")
```

## Data Filtering

Texas Filtering (Rows). Now let's filter for Texas data only. First, let me check what the State column looks like:

```{python}
usda_atlas['State'].unique()
county_demographics['State'].unique()
```

**Filter USDA Atlas for Texas**

```{python}
usda_texas = usda_atlas.query("State == 'Texas'")
print("Texas tracts:", usda_texas.shape)
usda_texas.head()
```

**Filter County Demographics for Texas**

```{python}
county_demographics_texas = county_demographics.query("State == 'Texas'")
print("Texas counties:", county_demographics_texas.shape)
county_demographics_texas.head()
```

**Keep all Food Nutrition data (not geographically specific)**


# Columns Filtering

We now have:

- usda_altas - usda data in Texas, California, New York
- county_demographics - counties data in Texas, California, New York
- food_nutrition - original(cleaned)

## Select & Rename Essential Columns

Now let's subset to only the essential columns needed for analysis and rename it for clear understanding

### USDA Food Access Atlas (10 columns)

* **PovertyRate** : Percentage of tract population living at or below federal poverty threshold
* **lapop1** : Population count beyond 1 mile from supermarket
* **lalowi1** : Low income population count beyond 1 mile from supermarket
* **lablack1** : Black/African American population count with low access
* **lahisp1** : Hispanic/Latino population count with low access
* **TractLOWI** : Total count of low-income population in tract

```{python}
# Rename for clear code

usda_columns_rename={
  'lapop1':'LowAccess_Pop',
  'lalowi1':'LowIncome_LowAccess',
  'lablack1':'Black_LowAccess',
  'lahisp1':'Hispanic_LowAccess',
  'TractLOWI':'Tract_LowIncome'
}


# Select and Rename 10 columns

usda_texas_subset = usda_texas.filter(items=['State', 'County', 'Urban', 'PovertyRate', 'Pop2010',
                                             'lapop1', 'lalowi1', 'lablack1', 'lahisp1', 'TractLOWI']
                              ).rename(columns = usda_columns_rename)
                              
print("USDA subset shape:", usda_texas_subset.shape)
```

```{python}
usda_texas_subset.head()
```

### County Demographics (10 columns)

* **Income.Median Houseold Income** :

This includes the income of the householder and all other individuals 15 years old and over in the household whether they are related to the householder or not. Because many households consist of only one person average household income is usually less than average family income.

* **Population.Population per Square Mile** :

Population density

```{python}
# Rename for clear code

county_columns_rename = {
    'Population.2010 Population': 'Population2010',
    'Income.Median Houseold Income': 'MedianIncome',
    'Education.Bachelor\'s Degree or Higher': 'Pct_BachelorsOrHigher',
    'Age.Percent Under 18 Years': 'Pct_Under18',
    'Ethnicities.Black Alone': 'Pct_Black',
    'Ethnicities.Hispanic or Latino': 'Pct_Hispanic',
    'Ethnicities.White Alone': 'Pct_White',
    'Population.Population per Square Mile': 'PopDensity'
}

# Select and Rename 10 columns

county_demographics_subset = county_demographics_texas.filter(items=['County', 'State', 'Population.2010 Population',
                                                                     'Income.Median Houseold Income',
                                                                     'Education.Bachelor\'s Degree or Higher',
                                                                     'Age.Percent Under 18 Years', 'Ethnicities.Black Alone',
                                                                     'Ethnicities.Hispanic or Latino', 'Ethnicities.White Alone',
                                                                     'Population.Population per Square Mile']
                                                      ).rename(columns=county_columns_rename)
print("Demographics subset shape:", county_demographics_subset.shape)
```

### Food Nutrition (9 columns)

```{python}

# Rename for clear code

food_columns_rename = {
    'Data.Protein': 'Protein',
    'Data.Carbohydrate': 'Carbohydrate',
    'Data.Fiber': 'Fiber',
    'Data.Sugar Total': 'SugarTotal',
    'Data.Fat.Total Lipid': 'TotalFat',
    'Data.Vitamins.Vitamin A - RAE': 'VitaminA',
    'Data.Major Minerals.Calcium': 'Calcium'
}

# Select and Rename 10 columns

food_nutrition_subset = food_nutrition.filter(items=['Category', 'Description', 'Data.Protein', 'Data.Carbohydrate',
                                                           'Data.Fiber', 'Data.Sugar Total', 'Data.Fat.Total Lipid',
                                                           'Data.Vitamins.Vitamin A - RAE', 'Data.Major Minerals.Calcium']
                                      ).rename(columns=food_columns_rename)
                                      
print("Food subset shape:", food_nutrition_subset.shape)
```



# Data Cleaning

We now have:

- usda_texas_subset - usda data filter 10 with renamed columns
- county_demographics_subset - counties filter 10 with renamed columns  
- food_nutrition_subset - food nutrition data filter 9 renamed columns

## Handle Missing Values

### usda_texas_subset

understand why drop this NULL and explain why need to drop

```{python}
usda_texas_subset.isnull().sum()
```

```{python}
usda_texas_cleaned = usda_texas_subset.dropna()
```

```{python}
usda_texas_cleaned.isnull().sum()
```

```{python}
print("Original rows:", usda_texas_subset.shape[0])
print("After removing NULLs:", usda_texas_cleaned.shape[0])
print("Rows removed:", usda_texas_subset.shape[0] - usda_texas_cleaned.shape[0])
```

**Check for zeros in key columns (zeros are valid - they mean "no population beyond threshold"):**

```{python}
print("Zeros in Pop2010:", (usda_texas_cleaned['Pop2010'] == 0).sum())
```

```{python}
print("Zeros in Tract_LowIncome:", (usda_texas_cleaned['Tract_LowIncome'] == 0).sum())
print("Zeros in LowIncome_LowAccess", (usda_texas_cleaned['LowIncome_LowAccess'] == 0).sum())

result = usda_texas_cleaned.query("Tract_LowIncome == 0 and LowIncome_LowAccess == 0")

print(f"Find {len(result)} rows data are both zero：")
print(result)
```
Here we drop both Tract_LowIncome and LowIncome_LowAccess are zero to make later calculation effectively.

```{python}
usda_texas_cleaned = usda_texas_cleaned.query("Tract_LowIncome != 0")

usda_texas_cleaned.shape
```

### county_demographics_subset

County Demographics - Check for -1 to NULL, then Remove:

```{python}
county_demographics_subset = county_demographics_subset.replace(-1, pd.NA)

county_demographics_subset.isnull().sum()
```

```{python}
county_demographics_cleaned = county_demographics_subset.dropna()
```

```{python}
print("Original rows:", county_demographics_subset.shape[0])
print("After removing NULLs:", county_demographics_cleaned.shape[0])
print("Rows removed:", county_demographics_subset.shape[0] - county_demographics_cleaned.shape[0])
```

### food_nutrtion_subset

Food Nutrition - Check and Remove NULL values:

```{python}
food_nutrition_subset.isnull().sum()
```

Since there's no missing value or NULL, we can just move to next step

## Verify Geographic Names

Now we have three clean datasets with NO missing values:

* usda_texas_cleaned
* county_demographics_cleaned
* food_nutrition_subset

Let's check the format of County names in both datasets

```{python}
usda_texas_cleaned['County'].head(10)
```

```{python}
county_demographics_cleaned['County'].head(10)
```

**Check sample counties from both:**

```{python}
print("USDA sample counties:", usda_texas_cleaned['County'].unique()[:5])
```

```{python}
print("Demographics sample counties:", county_demographics_cleaned['County'].unique()[:5])
```

**Check State format**

```{python}
print("USDA State format:", usda_texas_cleaned['State'].unique())
```

```{python}
print("Demographics State format:", county_demographics_cleaned['State'].unique())
```


**Check number of unique counties**

```{python}
print("Number of unique counties in USDA:", usda_texas_cleaned['County'].nunique())
```

```{python}
print("Number of unique counties in Demographics:", county_demographics_cleaned['County'].nunique())
```

Because we drop a NULL value in county_demographics dataset, so the counties would one less than USDA


# Data Transformation

We now have:

- usda_texas_cleaned - cleaned USDA data with standardized State
- county_demographics_cleaned - cleaned demographics data
- food_nutrition_subset - cleaned food data 

## Calculate Derived Metrics (USDA Atlas)

### Calculate PercentLowAccess (% of tract population >1 mile from supermarket)

What percentage of the tract's total population lives more than 1 mile from a supermarket?

```{python}
usda_with_metrics = usda_texas_cleaned.copy()

usda_with_metrics = (usda_with_metrics
    .assign(PercentLowAccess = lambda df_: (df_['LowAccess_Pop'] / df_['Pop2010']) * 100)
)

usda_with_metrics['PercentLowAccess'].describe()
```

Example:
- Census Tract A has POP2010 = 5,000 people total
- LowAccess_Pop = 1,500 people live >1 mile from supermarket
- PercentLowAccess = (1,500 / 5,000) × 100 = 30%
- Interpretation: 30% of this tract's population has low access to food

### PercentLowIncomeLowAccess (% of low-income population with low access):

What it means: Among the low-income population in this tract, what percentage also has low food access?

First, check for zero values in Tract_LowIncome (to avoid division by zero):

```{python}
print("Zeros in Tract_LowIncome:", (usda_with_metrics['Tract_LowIncome'] == 0).sum())
```

**Calculate the percentage, handling division by zero:**

```{python}
usda_with_metrics = (usda_with_metrics
    .assign(PercentLowIncomeLowAccess = lambda df_: (df_['LowIncome_LowAccess'] / df_['Tract_LowIncome']) * 100)
)

usda_with_metrics['PercentLowIncomeLowAccess'].describe()
```

```{python}
usda_with_metrics.isnull().sum()
```

```{python}
usda_with_metrics = usda_with_metrics.dropna()
```

Example:
* Census Tract B has Tract_LowIncome = 2,000 low-income people
* LowIncome_LowAccess = 800 low-income people with low access
* PercentLowIncomeLowAccess = (800 / 2,000) × 100 = 40%
* Interpretation: 40% of low-income residents have low food access

**Why These Calculations Matter**
Without percentages (raw counts only):

* Tract 1: 1,000 people with low access (sounds bad)
* Tract 2: 500 people with low access (sounds better)

**BUT if we look at percentages**

* Tract 1: 1,000 out of 10,000 = 10% low access (not too bad)
* Tract 2: 500 out of 1,000 = 50% low access (much worse!)

Percentages allow fair comparisons between large urban tracts and small rural tracts.

### Verify New Columns

```{python}
usda_with_metrics.columns.tolist()
```

## Categorize Nutrition (Food Nutrition)

### Group by Category and Calculate Mean Nutritional Values

```{python}
food_nutrition_grouped = (
    food_nutrition_subset
    .groupby('Category', as_index=False)
    .agg({
        'Protein': 'mean',
        'Carbohydrate': 'mean',
        'Fiber': 'mean',
        'SugarTotal': 'mean',
        'TotalFat': 'mean',
        'VitaminA': 'mean',
        'Calcium': 'mean'
    })
)

print(f"Shape: {food_nutrition_grouped.shape}")
food_nutrition_grouped = food_nutrition_grouped.round(1)
food_nutrition_grouped
```

### Wider/Longer

```{python}
food_nutrition_long = (
    food_nutrition_grouped.melt(
        id_vars=['Category'],
        var_name='Nutrient',
        value_name='MeanValue'
    )
)

print(f"Shape: {food_nutrition_long.shape}")
food_nutrition_long.head(10)
```

```{python}
# Then, use pivot() to create wide format with Nutrients as columns
food_nutrition_pivoted = (
    food_nutrition_long.pivot(
        index='Category',
        columns='Nutrient',
        values='MeanValue'
    )
    .reset_index()
)

print(f"Shape: {food_nutrition_pivoted.shape}")
food_nutrition_pivoted.head()
```

### Categorize Nutritional Values

[FDA Daily Value](https://www.fda.gov/food/nutrition-facts-label/daily-value-nutrition-and-supplement-facts-labels)

```{python}
# Import numpy for np.select
import numpy as np
```

#### SugarTotal

High sugar intake is linked to health issues; categorizing helps identify high-sugar vs low-sugar foods.

Thresholds:

* High: > 15 grams (>25% of 50g daily max)
* Medium: 5-15 grams
* Low: < 5 grams

```{python}
# Categorize SugarTotal
food_nutrition_pivoted = (
    food_nutrition_pivoted.assign(
        Sugar_Level = lambda df_: np.select(
            condlist=[
                df_['SugarTotal'] > 15,
                df_['SugarTotal'].between(5, 15, inclusive='both'),
                df_['SugarTotal'] < 5
            ],
            choicelist=['High', 'Medium', 'Low'],
            default='Unknown'
        )
    )
)

food_nutrition_pivoted.head()
```

#### TotalFat

Important for understanding nutritional quality and health implications.

Thresholds (per 100g serving):

* High: > 20 grams
* Medium: 5-20 grams
* Low: < 5 grams

```{python}
# Categorize TotalFat
food_nutrition_pivoted = (
    food_nutrition_pivoted.assign(
        Fat_Level = lambda df_: np.select(
            condlist=[
                df_['TotalFat'] > 20,
                df_['TotalFat'].between(5, 20, inclusive='both'),
                df_['TotalFat'] < 5
            ],
            choicelist=['High', 'Medium', 'Low'],
            default='Unknown'
        )
    )
)

food_nutrition_pivoted.head()
```

#### Protein

Helps identify protein-rich foods vs lower-protein options.

Thresholds (per 100g serving):

* High: > 15 grams (excellent protein source)
* Medium: 5-15 grams
* Low: < 5 grams

```{python}
# Categorize Protein
food_nutrition_pivoted = (
    food_nutrition_pivoted.assign(
        Protein_Level = lambda df_: np.select(
            condlist=[
                df_['Protein'] > 15,
                df_['Protein'].between(5, 15, inclusive='both'),
                df_['Protein'] < 5
            ],
            choicelist=['High', 'Medium', 'Low'],
            default='Unknown'
        )
    )
)

food_nutrition_pivoted.head()
```

#### Create Final Food_Nutrition dataset

```{python}
# Remove Protein, SugarTotal, and TotalFat columns, keep only their categorical levels
food_nutrition_final = food_nutrition_pivoted.drop(columns=['Protein', 'SugarTotal', 'TotalFat'])

print(f"Final shape: {food_nutrition_final.shape}")
print(food_nutrition_final.columns.tolist())

food_nutrition_final.head()
```

# Data Aggregation

We now have:

- usda_with_metrics - USDA data with two new columns (pct_lowaccess-> tracts have low access , pct_lowincomelowaccess -> low income with low access)
- county_demographics_cleaned - cleaned demographics data
- food_nutrition_final - final food nutrition data

## Aggregate USDA Atlas to County Level

```{python}
print("Number of tracts:", usda_with_metrics.shape[0])
print("Number of unique counties:", usda_with_metrics['County'].nunique())
```

```{python}
usda_county_agg = usda_with_metrics.groupby(['County', 'State']).agg({
    'Pop2010': 'sum',
    'LowAccess_Pop': 'sum',
    'Tract_LowIncome': 'sum',
    'LowIncome_LowAccess': 'sum',
    'Black_LowAccess': 'sum',
    'Hispanic_LowAccess': 'sum',
    'Urban': ['sum', 'count']
}).reset_index()
```

```{python}
usda_county_agg.columns = ['County', 'State', 'TotalPopulation', 'TotalLowAccessPop',
                             'TotalLowIncomePop', 'TotalLowIncomeLowAccessPop',
                             'TotalBlackLowAccess', 'TotalHispanicLowAccess',
                             'CountUrbanTracts', 'TotalTracts']
```

```{python}
usda_county_agg.columns.tolist()
usda_county_agg.head()
```

## Calculate Weighted Average Poverty Rate

**Purpose**

* Calculate a county-level poverty rate that accurately represents the entire county population by weighting each census tract's poverty rate by its population size.

```{python}
# Step 5.2: Calculate Weighted Average Poverty Rate

# Create intermediate columns for weighted calculation using .assign()
usda_weighted_prep = usda_with_metrics.assign(
    PovertyWeighted = lambda df_: df_['PovertyRate'] * df_['Pop2010'],
    PopWeight = lambda df_: df_['Pop2010']
)
```

```{python}
# Group by County and State, then calculate weighted average
poverty_weighted = (usda_weighted_prep
                    .groupby(['County', 'State'], as_index=False)
                    .agg({
                        'PovertyWeighted': 'sum',
                        'PopWeight': 'sum'})
                    .assign(
                        AvgPovertyRate = lambda df_: df_['PovertyWeighted'] / df_['PopWeight']
                    )
                    [['County', 'State', 'AvgPovertyRate']]
                   )
```

**Process**
1. We used the .assign() method to create two intermediate columns: PovertyWeighted (poverty rate multiplied by population) and PopWeight (population).
2. We grouped the data by County and State, summed these weighted values, and calculated the final weighted average poverty rate.
3. This approach ensures that larger census tracts have proportionally more influence on the county's overall poverty rate, giving us a more accurate representation than a simple average would provide.

```{python}
print("Weighted poverty rate calculated.")
print(f"Shape: {poverty_weighted.shape}")
poverty_weighted.head()
```

**Outcome**

A clean dataset with 254 Texas counties, each with a single, population-weighted poverty rate that can be used for county-level analysis.

## Add Derived Percentage Metrics to County Aggregation

**Purpose:**

Transform raw population counts into meaningful percentages that allow for fair comparisons between counties of different sizes.

**Exceptional Handling**

Before we ```assign```new columns, we should examine whether any abnormal values exist? (Ex. TotalLowAccessPop > TotalPopulation)

```{python}
# We use query to pick up rows that are reasonable
usda_county_agg_cleaned = usda_county_agg.query("TotalLowAccessPop < TotalPopulation and TotalLowIncomeLowAccessPop < TotalLowIncomePop")

print("Original rows:", usda_county_agg.shape[0])
print("After removing NULLs:", usda_county_agg_cleaned.shape[0])
print("Rows removed:", usda_county_agg.shape[0] - usda_county_agg_cleaned.shape[0])
```
Then we start to add Derived Percentage Metrics

```{python}
# Add Derived Percentage Metrics

# Create percentage columns using .assign() with lambda
usda_county_agg_final = (usda_county_agg_cleaned
                         .assign(
                             Pct_LowAccess = lambda df: (df['TotalLowAccessPop'] / df['TotalPopulation']) * 100,
                             Pct_LowIncome_LowAccess = lambda df: (df['TotalLowIncomeLowAccessPop'] / df['TotalLowIncomePop']) * 100,
                             Pct_Urban = lambda df: (df['CountUrbanTracts'] / df['TotalTracts']) * 100
                         )
                        )
```

```{python}
# Merge with weighted poverty rate
usda_county_agg_final = usda_county_agg_final.merge(
    poverty_weighted,
    on=['County', 'State'],
    how='left'
)
```

**Process:**

Using the .assign() method with lambda functions, we calculated three key percentage metrics:

1. Pct_LowAccess: The percentage of the county population living more than 1 mile from a supermarket
2. Pct_LowIncome_LowAccess: The percentage of low-income residents who also face low food access
3. Pct_Urban: The percentage of census tracts in the county classified as urban


```{python}
print("Derived metrics added.")
print(f"Shape: {usda_county_agg_final.shape}")
usda_county_agg_final.head()
```

**Outcome**

A dataset where all counties can be directly compared regardless of their population size, making patterns in food access and poverty more visible.


# Data Merging

We now have:

- **usda_county_agg_final** - USDA data covers population(overall, lowaccess,income), race/ethnicity, percentage of low accessincome, urban/rural,poverty
- **county_demographics_cleaned** - cleaned demographics data
- **food_nutrition_final** - final food nutrition data

## Prepare County Demographics

```{python}
# Select and rename columns for clarity
county_demo_final = county_demographics_cleaned.reset_index(drop=True).copy()

print(f"Shape: {county_demo_final.shape}")
county_demo_final.head()
```

## Merge USDA and County Demographics

```{python}
# Merge on County and State
texas_county_merged = usda_county_agg_final.merge(
    county_demo_final,
    on=['County', 'State'],
    how='inner'
)

print(f"Shape: {texas_county_merged.shape}")
print(f"Columns: {texas_county_merged.columns.tolist()}")
texas_county_merged.head()
```

**Process**

***inner merge*** on County and State columns, ensuring that only counties present in both datasets were included.

This merge operation links food access metrics (like percentage of population with low access) with demographic factors (like median income and education levels).

**Outcome**

A unified dataset containing both food access metrics and demographic information for 254 Texas counties.

## Select Only Essential Columns for Final Dataset

**Process**

We carefully selected columns that directly address our research questions about food access disparities:

* <u>Geographic identifiers</u>: County, State
* <u>Population</u>: TotalPopulation, PopDensity
* <u>Food Access</u>: Pct_LowAccess, Pct_LowIncome_LowAccess, Pct_Urban
* <u>Economic</u>: AvgPovertyRate, MedianIncome
* <u>Education</u>: Pct_BachelorsOrHigher
* <u>Demographics</u>: Pct_Hispanic, Pct_Black

```{python}
# Keep only the 12 most important columns for analysis

texas_county_final = texas_county_merged.filter(items = ['County', 'State', 'TotalPopulation',
                   'Pct_LowAccess', 'Pct_LowIncome_LowAccess', 'Pct_Urban',
                   'AvgPovertyRate', 'MedianIncome', 'Pct_BachelorsOrHigher',
                   'Pct_Hispanic', 'Pct_Black', 'PopDensity']).copy()

print("Essential columns selected.")
print(f"Final shape: {texas_county_final.shape}")
print(f"Columns ({len(texas_county_final.columns)}): {texas_county_final.columns.tolist()}")
```

**Outcome**

A streamlined, analysis-ready dataset with only the columns needed to answer questions about urban vs. rural disparities, poverty-access correlations, and demographic differences in food access.

# Dataset Preparation

## Save to "Texas County Merged Dataset"

```{python}
# Step 7.1: Save Texas_County_Merged.csv

texas_county_final.to_csv('Texas_County_Merged.csv', index=False)

print(f"  Shape: {texas_county_final.shape}")
print(f"  Columns: {len(texas_county_final.columns)}")
texas_county_final.head()
```

## Save to "Food_Nutrition_Final"

```{python}
# Save Food Nutrition dataset
food_nutrition_final.to_csv('Food_Nutrition_Final.csv', index=False)

print(f"  Shape: {food_nutrition_final.shape}")
print(f"  Columns: {len(food_nutrition_final.columns)}")
food_nutrition_final.head()
```

# Analysis with Visualizations

```{python}
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```

## Examining the Poverty-Food Access Relationship

```{python}
# Load the dataset
texas_county = pd.read_csv('Texas_County_Merged.csv')

# Remove any rows with missing values in the variables of interest
analysis_data = texas_county.loc[texas_county['AvgPovertyRate'].notna() &
                                  texas_county['Pct_LowIncome_LowAccess'].notna()].copy()

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(analysis_data['AvgPovertyRate'],
            analysis_data['Pct_LowIncome_LowAccess'],
            alpha=0.6,
            s=50,
            color='#F18F01')

# Add trend line using numpy polyfit
z = np.polyfit(analysis_data['AvgPovertyRate'], analysis_data['Pct_LowIncome_LowAccess'], 1)
p = np.poly1d(z)
plt.plot(analysis_data['AvgPovertyRate'],
         p(analysis_data['AvgPovertyRate']),
         "r--",
         linewidth=2,
         label=f'Trend Line')

plt.xlabel('Average Poverty Rate (%)', fontsize=12)
plt.ylabel('% Low-Income Population with Low Access', fontsize=12)
plt.title('Relationship Between Poverty and Food Access in Texas Counties',
          fontsize=14,
          fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Calculate correlation coefficient
correlation = analysis_data['AvgPovertyRate'].corr(analysis_data['Pct_LowIncome_LowAccess'])
print(f"\nCorrelation between Poverty Rate and Low-Income Low Access: {correlation:.3f}")
print(f"Number of counties analyzed: {len(analysis_data)}")
```


## Takeaways from these charts

1. **Weak negative correlation (-0.073)**: Contrary to intuitive expectations, there is virtually no linear relationship between poverty rates and low-income food access in Texas counties. The trend line is nearly flat with a slight downward slope.

2. **High variability across all poverty levels**: Counties with similar poverty rates show dramatically different food access outcomes. For example, counties with ~15% poverty rates range from 20% to 110% low-income low access.

3. **Clustering around 50-70% food access**: Most counties fall in the 40-80% range for low-income food access regardless of poverty rate, suggesting other factors (geography, infrastructure, transportation) may be more influential than poverty alone.

4. **Outliers at low poverty**: Several counties with poverty rates below 15% still have 90-140% low-income low access, indicating that even relatively affluent counties can face significant food access challenges.

5. **No floor or ceiling effect**: Counties exist across the full spectrum of both variables, with no obvious threshold where food access dramatically improves or worsens.

# Challenge and How We Approached

Working through this three dataset food accessibility project presented a steady stream of challenges, from initial data exploration through the final merge operations. Though the individual steps weren't conceptually revolutionary, the cumulative effect of working with three large, messy datasets taught us that data wrangling is fundamentally about systematic problem-solving.

Our first major challenge hit us during the initial data loading step. The USDA Food Access Research Atlas arrived with over 72,000 rows and 147 columns. We couldn't immediately tell which columns mattered for us Texas-focused data wrangling, and the huge volume made it tempting to just grab everything and figure it out later. Instead, We forced to slow down and examine the data dictionary, identifying which accessibility metrics actually aligned with my research questions about food deserts and low-income populations. This disciplined subsetting approach—reducing from 147 columns down to just 10-15 core variables—became a pattern we repeated with the County Demographics (43 columns) and Food Nutrition (38 columns) datasets. By the end, we had achieved an 86.8% dimension reduction across all three datasets, transforming 228 total columns into just 30 variables.

Once we had manageable subsets, we encountered our next obstacle, which is inconsistent missing value representations. The County Demographics dataset used -1 as a missing value placeholder, the USDA Atlas mixed genuine zeros with missing data markers, and the Food Nutrition dataset contained standard NaN values. We couldn't simply apply one universal cleaning rule. Instead, we developed dataset-specific filtering out -1 values from demographics, carefully distinguishing between "no low-access population" (legitimate zero) versus "data not collected" (missing) in the food access data, and handling NaN values in the nutrition dataset. This taught us that missing data isn't a single problem with a single solution; it requires understanding what missingness means in each specific context.

The census tract-to-county mapping challenge emerged during our merge planning phase. The Food Access Atlas operates at the census tract level, while County Demographics works with county-level aggregates. We initially worried about losing by aggregating tracts up to counties, but we realized this was actually necessary for our wrangling scope. The real challenge became ensuring we didn't accidentally duplicate data during the aggregation process or lose tracts that didn't cleanly map to our Texas county subset. We solved this by filtering for Texas first at the tract level, then carefully aggregating using sum functions for population counts and mean functions for rate-based metrics, always verifying our row counts before and after each transformation step.

Perhaps our most frustrating challenge involved inconsistent naming conventions across datasets. County names appeared with and without "County" suffixes, state representations alternated between full names and abbreviations, and the Food Nutrition dataset contained food name variations that seemed designed to confound any analysis. Early on, we attempted to handle these variations through complex pattern matching during data import, building what essentially became a mental dictionary of name pairs. The breakthrough came when we realized we needed to standardize these values explicitly through dedicated transformation steps rather than trying to match variations during merges. For counties, we stripped suffixes and converted to title case. For states, we created a consistent Texas filter that could handle both "TX" and "Texas" representations.

Our final major challenge involved determining the correct merge sequence and types. With three datasets coming from completely different sources, we had to map out which variables could serve as keys. We settled on a two-track approach: merging Food Access and County Demographics on county and state names (many-to-one, since multiple tracts map to each county), while keeping Food Nutrition as a standalone dataset that could later be linked through categorical analysis rather than direct merges. The trickiest part was ensuring we used inner joins to maintain only Texas-specific records while not accidentally dropping valid data due to name mismatches we hadn't yet caught.

Looking back, we are particularly proud of an aspect of this project.

* Our reduction strategy not only made the datasets manageable but actually improved their wrangling clarity. Rather than keeping every possible variable "just in case," we made deliberate choices about which metrics truly addressed my research questions, resulting in three clean, focused datasets that tell a coherent story about Texas food accessibility.

Throughout this project, we learned that data wrangling excellence isn't about techniques or elegant one-liners. It's about systematic thinking, attention to detail, and the patience to verify each transformation step before moving forward. The encoding inconsistencies and subtle aggregation errors really taught me that success in data wrangling comes from developing a suspicious, verification-oriented mindset where you trust nothing until you've explicitly confirmed it works as intended.

# Description of tool learning 

First, we learned to use the Pandas library by reviewing course materials on DataFrame manipulation. Once we understood the basics of data structures, we explored the Pandas documentation to master merging datasets using ```pd.merge()``` and handling missing values. A specific challenge we encountered was calculating the weighted average for poverty rates across census tracts, we learned from course materials how to use the ```.assign()``` function with lambda expressions to perform these complex row-wise calculations efficiently. Second, we learned to visualize data using Matplotlib. We consulted the documentation to understand how to create scatter plots and searched on Q&A platforms to find out how to overlay a trend line (np.polyfit) to effectively communicate the correlation between poverty and food access. These tools were essential for the analysis of the project.
